# Tokenization

- why?
- 3 types of tokens: char, word, subwords
    - word: 1) OOV 2) Long tail effect 3) wont learn the similar meanings behind swim and swimming.
    - char: sequence length too long. expensive compute
    - subwords units.
- Subwords Methodologies
    -  1) first, u get 26 chars and punc, training corpus, and a required vocab size V. 2) split all tokens. e.g. cat [c_, _a_, _t, ca_, _at, cat].
    3) the question is: how to merge permutation units to |V| important units s.t. u have a best representation of the corpus? How to measure which subwords are the most important? You need to construct an importance score.
    - 3 methods
        - [BPE](https://arxiv.org/pdf/1508.07909) / Byte-level BPE.
            merge on frequency. count all freq of top units. merge all subwords that can reduce freq of each subwords and update vocab. start from top freq units (usually start from chars.)
            [code](1.1.bpe.py)
        - BBPE: Byte-level BPE.
            - intead of using string. use bytes.
            - pros:
                - BPE enables multi-lingual learnings.
                - compression
                - not just for text encoding and tokenization. can also work on images etc.
        - Wordpiece: find |V| subwords that gives the best log likelihood of the training corpus S.
            ```
            token x, y --> token z
            $$logP(z) - (logP(x)+logP(y)) = log\frac{P(x)}{P(x)P(y)}$$
            ```
        - Unigram Language Model
            - How to score if a token is important? A token is important if deleting the sub-units would cause more loss to the unigram model. This is still getting the top freq unigram model from a text corpus. 
            - keep single char. to avoid OOV.
            ```
            p(x) = \prod p(x_i). i = len(get_unigram(corpus)) 
            x_hat = arg max p(x)
            ```

- Comparisons
    - bpe and wordpiece: how to merge? from small vocab to big.
    - uigram lm: how to use lm to shrink vocab from big to small?

- tokenization is the roof of all evil, bc its next token prediction, u cant use llm for arithmetic computing.

# Embedding

- Why? word embedding. dense representation can get semantic mearnings between words. It's basically training MLP to get one-hot encoding vectors into dense vectors to represent word meanings. 
- lookup table `nn.Embedding(vocab_size, embedding_dim)` turn |V| vector into 512/768 dim.
- everything is a mapping. u can map a word into vector. u can map sentence into vector. u can also map tokens into high dim embedding space. it's more like how can u train that model?
- embedding 单射且同构
- Methods
    - one-hot
        - problems: dim explosion. one-hot encoding. every token is orthogonal. u cant learn the semantic meanings between words.
        - in some use cases, u can do one-hot encoding + PCA.
        - why we need to use one-hot? we turned discrete features into continuous. each feature is orthogonal
    - distributed representation: But u need to turn one-hot encoding features into Euclidean space so u can measure LP distance.
    
    - word2vec
        - 
    - fast-text

# Attention

- Why?
- self-attention
- multi-head self-attention
- sparse attention
- linear attention
- KV cache
- multi-head attention
- multi-query attention
- grouped-query attention
- multihead latent attention

# FFN Add LN

- why FFN?
- why add residual?
- why layer norm?
- how to do layer norm correctly?
- why batch norm?
- what types of activations
    - sigmoid
    - tanh
    - relu
    - leaky relu
    - elu
    - swish
    - swiglu
    - softmax

# Postional Encoding
- why?
- absolute positional encoding
- relative positional encoding
- RoPE
- ALiBi
- 长度外推优化

# Structure and Decoding
- llm architecture
- decoding strategies
    - greedy search
    - beam search
    - top-k sampling
    - top-p sampling
    - temperature sampling
    - KPT
    - Best of N
    - Majority vote
    - self-consistency