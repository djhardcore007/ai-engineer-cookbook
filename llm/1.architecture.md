# Tokenization

- why?
- 3 types of tokens: char, word, subwords
    - word: 1) OOV 2) Long tail effect 3) wont learn the similar meanings behind swim and swimming.
    - char: sequence length too long. expensive compute
    - subwords units.
- Subwords Methodologies
    -  1) first, u get 26 chars and punc, training corpus, and a required vocab size V. 2) split all tokens. e.g. cat [c_, _a_, _t, ca_, _at, cat].
    3) the question is: how to merge permutation units to |V| important units s.t. u have a best representation of the corpus? How to measure which subwords are the most important? You need to construct an importance score.
    - 3 methods
        - [BPE](https://arxiv.org/pdf/1508.07909) / Byte-level BPE.
            merge on frequency. count all freq of top units. merge all subwords that can reduce freq of each subwords and update vocab. start from top freq units (usually start from chars.)
            [code](1.1.bpe.py)
        - BBPE: Byte-level BPE.
            - intead of using string. use bytes.
            - pros:
                - BPE enables multi-lingual learnings.
                - compression
                - not just for text encoding and tokenization. can also work on images etc.
        - Wordpiece: find |V| subwords that gives the best log likelihood of the training corpus S.
            ```
            token x, y --> token z
            $$logP(z) - (logP(x)+logP(y)) = log\frac{P(x)}{P(x)P(y)}$$
            ```
        - Unigram Language Model
            - How to score if a token is important? A token is important if deleting the sub-units would cause more loss to the unigram model. This is still getting the top freq unigram model from a text corpus. 
            - keep single char. to avoid OOV.
            ```
            p(x) = \prod p(x_i). i = len(get_unigram(corpus)) 
            x_hat = arg max p(x)
            ```

- Comparisons
    - bpe and wordpiece: how to merge? from small vocab to big.
    - uigram lm: how to use lm to shrink vocab from big to small?

- tokenization is the roof of all evil, bc its next token prediction, u cant use llm for arithmetic computing.

# Embedding

- Why? word embedding. dense representation can get semantic mearnings between words. It's basically training MLP to get one-hot encoding vectors into dense vectors to represent word meanings. 
- lookup table `nn.Embedding(vocab_size, embedding_dim)` turn |V| vector into 512/768 dim.
- everything is a mapping. u can map a word into vector. u can map sentence into vector. u can also map tokens into high dim embedding space. it's more like how can u train that model?
- embedding 单射且同构
- Methods
    - one-hot
        - problems: dim explosion. one-hot encoding. every token is orthogonal. u cant learn the semantic meanings between words.
        - in some use cases, u can do one-hot encoding + PCA.
        - why we need to use one-hot? we turned discrete features into continuous. each feature is orthogonal
    - distributed representation: But u need to turn one-hot encoding features into Euclidean space so u can measure LP distance. dense representation is 1) easy to measur
    
    - word2vec: predict target word given context words.
        - cbow (contiuous bag of words)
            - Given the words around the blank, what word should be in the blank?
            - avg over context words' embeddings
            - ignores order.

            1. Context Words
                
                Context words: w_1, w_2, ..., w_c
                
                Target word: w_t  
            2. One-Hot Encoding

                Each word is a one-hot vector of size V (vocabulary size):
                $$
                            [
                            x_i \in \mathbb{R}^V
                            ]
                $$            
            3. Embedding Lookup

                Embedding matrix:
                $$
                [
                W \in \mathbb{R}^{V \times D}
                ]
                $$
                Word embedding for context word (w_i):
                $$
                [
                v_i = W^T x_i
                ]
                $$
                (Just selects the word’s embedding row.)
            
            4. Hidden layer: Average the Context Vectors
            $$

            [
            h = \frac{1}{C} \sum_{i=1}^{C} v_i
            ]
            $$
            5. Output Layer
            
                Second matrix:
                $$
                [
                W' \in \mathbb{R}^{D \times V}
                ]
                $$
                Scores for all words:
                $$
                [
                u = {W'}^T h
                ]
                $$
                
            6. Softmax Prediction
            $$
            [
            p(w_t \mid context)=
            \frac{\exp(u_t)}{\sum_{j=1}^{V} \exp(u_j)}
            ]
            $$
            Maximize the probability of the target word:
            $$
            [
            \max \log p(w_t \mid context)
            ]
            $$
            (Usually approximated with [negative sampling](https://en.wikipedia.org/wiki/Word2vec))
    - skip-gram: Predict context words FROM the target word.
    - fast-text: FastText still uses CBOW or Skip-Gram, but replaces the word embedding with the sum of its character n-gram vectors.
        
        Given word `playing` nrgams `n = 3`
        `<pl, pla, lay, ayi, yin, ing, ng>` + `<playing>` (basically a sliding window size = 3)
        embedding of the word playing is now
        $$
        [
        v_{\text{playing}} = \sum_{g \in G} z_g
        ]
        $$
        Where:
        - (G) = set of character n-grams of the word
        - (z_g) = embedding vector of each n-gram

        Then apply cbow or skip gram on top.
        $$
        [
        \max \sum_{c \in context} \log p\left(c \mid \text{ngrams}(w_t)\right)
        ]
        $$
        this works bc granularity better s.t. 1. less oov. 2. more efficience bc vocab size drops.

- word2vec how to accelerate?
    - hierarchical softmax
        - binary tree. complexity: |V| -> logV. 
        - higher freq words are located closer to the roots.
        - "greedy optimization"
        - cons: if the word is rare. u have to go deep into the branches.
    - negative sampling
        - def: a training trick used in word2vec (CBOW & Skip-Gram) to avoid computing a slow full softmax over the entire vocabulary.
        - why? faster and more efficient.
        - how?
            - The full softmax requires:
                $$
                [
                \text{sum over all } V \text{ words}
                ]
                $$
                If vocabulary size is big, this is extremely slow.
            - Negative sampling replaces this expensive softmax with a binary classification task. how to construct?
                - 1 positive example  (the true target/context word)
                    ```
                    (target = "cat", context = "cute")
                    ```
                - K negative examples (random wrong words)
                    ```
                    ("cat", "banana")
                    ("cat", "engine")
                    ("cat", "chair")
                    ```
            - maths

                Positive example:
                $$
                [
                \log \sigma (v_{context}^\top v_{target})
                ]
                $$
                Negative examples:
                $$
                [
                \sum_{i=1}^{K} \log \sigma (-v_{neg_i}^\top v_{target})
                ]
                $$
                Total loss:
                $$
                [
                L = \log \sigma (v_{+}^\top v_{t})

                * \sum_{i=1}^{K} \log \sigma (-v_{neg_i}^\top v_{t})
                ]
                $$
                Where:
                - (\sigma) = sigmoid
                - (v_+) = embedding of the real context word
                - (v_{neg_i}) = embeddings of negative words


# Attention

- Why?
    - core idea: assign importance score to different tokens. focus on important tokens.
    - handles long sequence better by keeping order info.
    - parallelization. efficient compute.
- self-attention
    - modules
        - vocab embedding: what am I? what word meaning do I have?
        - positional embdding: where am I?
        - query: what `info` are u looking for? we capture that question using query matrix.
        - key
        - value: use `k * v` to construct importance score. The more matching we r, the bigger the score.
        - importance score becomes
            $$
            attention = softmax \frac{Q K ^T}{\sqrt{dim_k}} \cdot V
            $$
        - why softmax? make sure the score falls within a range [0,1]. It turns the score to a distance measure. 
        - why scale? control variance. If dim_k too big, u importance score gets closer to 0 or 1. in gradient descent, its less harder to converge.
- self attention / encoder attention: full attention. study the importance score between that token vs the rest.
- decoder attention : masked / causal attention. auto-regressive. prevents data leakage.
- cross attention: query comes from input sequence 1, key and value come from input sequence 2.
    - In machine translation, such relationship between English and Chinese are super useful.
- multi-head self-attention
    - add more hidden layers. adds model complexity.
- Complexity: $$O(N^2)$$ where $N$ is num tokens per sequence.
- sparse attention
    - how? It only study the importance relationships at the cloest `k` tokens given a token. sliding window. this is like adding a constraint / prior to the model.
    - by controling the attention mask. u can control locality etc.
    - cuz u "hand engineer" the attention mask. its not "automated" can be more stupid. u should "learn" the attention mask.
    - [code](1.3.sparse_attention.py)
- linear attention
    - how? swap softmax into linear kernels.
    - why? drops complexity from $O(N^2)$ to $O(N)$.
    - math

        Softmax prevents factorization
        $$
        \text{softmax}(QK^\top)
        $$
        cannot be written as a product of simple matrices.
        It inherently requires building the full $O(N^2)$ attention matrix.
        Kernel feature maps *can* be factorized
        If we use a feature map $\phi$:
        $$
        \phi (QK^\top)  = \phi(Q)\phi(K)^\top
        $$
        we can reorder operations.
        instead of:
        $$
        softmax(Q(K)ᵀ) \cdot V → O(n²)
        $$
        we compute:
        $$
        \phi(Q)\phi(K)^\top V → O(n)
        $$
        where $\phi$ is a kernel feature map:
        | φ(x)             | Range     | Positive?    |
        | ---------------- | --------- | ------------ |
        | ReLU(x) + 1      | ([1, ∞))  | Yes          |
        | ReLU(x)²         | ([0, ∞))  | Yes          |
        | ELU(x) + 1       | ((0, ∞))  | Yes ✓ (best) |
        | LeakyReLU(x) + 1 | ((−∞, ∞)) | No           |
        | exp(x)           | ((0, ∞))  | Yes ✓        |
        | cos / sin        | ([-1, 1]) | No           |
        | x                | ((−∞, ∞)) | No           |
        | x²               | ([0, ∞))  | Yes          |

        as long as the kernel can be factorized and positive and smooth. (good for optimization)

        Now no O($N \cdot N$) matrix → no quadratic cost → linear time

    - downside:
        - We lose the normalization effects of softmax
        - Attention distribution becomes less sharp
        - Sometimes lower quality on tasks requiring exact token interactions
        - Softmax enforces a probability distribution over attention weights—linear attention does not.

    - [implementaton of linear attention in transformers](https://github.com/lucidrains/linear-attention-transformer)
- KV cache
    - mainstream attention: masked self-attention. auto-regressive. it uses all K,V of previous tokens. to prevent recalculating previous tokens' K,V. so we just `cache` previous K,V for efficiency.
- [inference framework lmdeploy](https://github.com/InternLM/lmdeploy)
- flash-attention
    - why? for longer sequence. u split the sequence into chunks. they parallel KV compute for each chunk. then "aggregate"/"reduce" results to original sequence.
    - how? map-reduce. 
        1. Chunk the sequence into blocks (`tiles`).
        2. For each block: Load Q, K, V tiles into high-speed SRAM. Compute attention scores and partial outputs.
        3. Reduce the partial results to final output.
    - This avoids materializing the full attention matrix.
    - flash-attention leads to `flash-decoding`.
    - Compute attention incrementally, tile by tile.
    - Keep intermediary data on-chip (GPU shared memory).
    - Use numerically stable softmax with running maxima.
- streamingLLM
    - masked self-attention + tiles to enable auto-regressive features
- MHA multi-head attention
    - why multiple heads? Similar to group `convolution`. It’s like creating many communication channels, with each channel focusing on different details. It’s similar to adding convolution operations on top of attention. Each head can focus on features from **different subspaces of the sequence**. each head has different semantic meanings:
        ```
        head 1: positional patterns
        head 2: syntax
        head 3: long-range links
        head 4: entities / names
        ```
- MQA multi-query attention
    - how it works?
        - Multiple queries, Shared keys, Shared values
        - Q still has H heads. But K and V are single-headed (shared).
    - why?
        - efficient in large scale inference. w a little bit of performance reduction.
- grouped-query attention
    - Why? MQA is too extreme (1 group). MHA is too expensive (H groups). so in between, you have G groups of queries. each group of Q has multiple heads. each group share same K and V.
    - u keep good quality and maintain speed.
- multihead latent attention
    - combines multihead attention and linear attention.
- DCA (Dual chunk attention)
    - hows?
        1. split long-form sequence into chunks.
        2. calculate attention within a chunk
        3. calculate cross-block attention: attention between chunks.
        4. concat results.
    - why?
        - efficient compute.
        - less RAM cost.
- S2 attention:
    - how? 

# FFN Add LN

- why FFN?
    - after MHA, tokens check each other out. FFN is for computing and thinking.
- why add residual?
    - avoid gradient explosion. explosion happens when gradient > 1. the deeper, the easier to explode. add a residual adds connections between layers.
    - standard residual: add express lane for information.
    Here is a **simple Markdown table** of the main residual block variants, with clean math.

        | Method                  | Formula                                                                                                                                          | Notes                                                          |
        | ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------- |
        | **Standard Residual**   | $$x_{l+1} = x_l + f(x_l)$$                                                                                                                       | Original ResNet-style skip connection                          |
        | **Post-LN Transformer** | $$x_{l+1} = \text{LN}(x_l + f(x_l))$$                                                                                                            | Used in early Transformers (BERT), less stable for deep models |
        | **Pre-LN Transformer**  | $$x_{l+1} = x_l + f(\text{LN}(x_l))$$                                                                                                            | Used in all modern LLMs (GPT, LLaMA), very stable              |
        | **DeepNorm**            | $$x_{l+1} = x_l + \alpha \cdot f(\text{LN}(x_l))$$ <br> Encoder: $$\alpha = \frac{1}{\sqrt{2N}}$$ <br> Decoder: $$\alpha = \frac{1}{\sqrt{4N}}$$ | Adds residual scaling so **very deep** models stay stable  $N$ is # stacks.    |
        | **ReZero**              | $$x_{l+1} = x_l + \alpha \cdot f(x_l)$$ (α learnable, init = 0)                                                                                  | Starts with zero residual; stabilizes early training           |


- normalizations
    - why? stable convergence. avoid gradient diminishing or exploding.
    - types
        - layer norm
            - norm over `feature dimension=hidden size = embedding size = token vector size` (e.g. 512/768). 
            - mainly for RNN, Transformers. LayerNorm works per token, across its features. so each token would be normalized over the mean of 768 values and variance of 768 values.
            - LayerNorm does NOT look across tokens. why? u want each token emb on the same scale.
            - u can never norm across tokens. why? tokens are semantically different. u can only norm within the 768 features for a single token.
            - if u have 10 tokens in a sentence. u need to norm 10 times.
        - batch norm
            - norm over `batch_size`
            - BatchNorm looks across examples in the batch.
            - mainly for CNN. why? u want 2 faces on the same scale.
        - how to do norm correctly?

            | Method                | Formula                                                                                                                                                                          | What It Normalizes / Changes                                        | Notes                                                             |
            | --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- | ----------------------------------------------------------------- |
            | **Vanilla LayerNorm** | $$(\displaystyle x_{\text{norm}} = \frac{x - \mathbb{E}[x]}{\sqrt{\text{Var}(x)+\epsilon}} \cdot \gamma + \beta)          $$                                                         | Normalizes **per token across features** (mean + variance)          | Standard LN used in early Transformers                            |
            | **RMSNorm**           | $$(\displaystyle \text{RMS}(x)=\sqrt{\frac{1}{H}\sum_{i=1}^{H} x_i^2 + \epsilon}) $$ $$  (\displaystyle x_{\text{norm}} = \frac{x}{\text{RMS}(x)} \cdot \gamma)   $$                   | Normalizes only by **root mean square** (no mean subtraction, no β) | Faster, more stable; used by LLaMA, T5, **No need to shift, only need to scale.**                          |
            | **DeepNorm**          | $$(\displaystyle x_{l+1} = x_l + \alpha \cdot f(\text{LN}(x_l))) $$ $$  Encoder: (\displaystyle \alpha=\frac{1}{\sqrt{2N}}) $$ $$  Decoder: (\displaystyle \alpha=\frac{1}{\sqrt{4N}}) $$| **Scales residual branches**, not normalization itself              | Enables very **deep** transformers (hundreds–thousands of layers) |

# FNN + Activations

- vanilla FNN activations 
    $$
    FNN(x) = Relu(xW_1 + b_1)W_2 + b_2
    $$
- GLU FNN activation
    $$
    GLU(x) = x V \cdot (xW+b)
    $$

    $$
    FNN_{GLU}(x) = x V \cdot \sigma(x W_1 +b) W_2
    $$
    
    why this works? activation is like gate control. $\sigma$ is like the gate to control which affine transformation gets through.

    If u swap $\sigma$ with `Swish`, `GeLU` u get the newest architecture design in current LLM.
## types of activation

| Activation     | Formula                                                                        | Notes                                                                                             |   |                                 |
| -------------- | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------- | - | ------------------------------- |
| **Sigmoid**    | $$\sigma(x) = \frac{1}{1 + e^{-x}}$$                                           | Causes **vanishing/exploding gradients**; saturates for large (                                   | x | ); expensive due to exponential |
| **Tanh**       | $$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$                               | Still suffers gradient vanishing; exponential compute; zero-centered                              |   |                                 |
| **ReLU**       | $$\text{ReLU}(x) = \max(0, x)$$                                                | No gradient vanishing for (x>0); not smooth; gradients = 0 for negative values; not zero-centered |   |                                 |
| **Leaky ReLU** | $$\text{LeakyReLU}(x) = \begin{cases}x, & x>0 \ \alpha x, & x<0 \end{cases}$$  | Prevents dying ReLU; still quite linear; simple compute                                           |   |                                 |
| **ELU**        | $$\text{ELU}(x) = \begin{cases}x, & x>0 \ \alpha(e^x - 1), & x<0 \end{cases}$$ | Smooth; negative values help shift mean; exponential = costly                                     |   |                                 |
| **Swish**      | $$\text{Swish}(x) = x \cdot \sigma(x)$$                                        | Smooth, non-monotonic; excellent performance in deep nets                                         |   |                                 |
| **GELU**       | $$\text{GELU}(x) = x \cdot \Phi(x)$$ <br>((\Phi) = Gaussian CDF)               | Used in Transformers; smooth, probabilistic gating behavior                                       |   |                                 |
| **SwiGLU**     | $$\text{SwiGLU}(x) = \text{Swish}(x_1) \odot x_2$$                             | Most effective MLP activation in modern LLMs                                                      |   |                                 |
| **Softmax**    | $$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$$                       | Converts logits to a **probability distribution**                                                 |   |                                 |


# Postional Encoding
- why? transformers doesnt learn orders.
- how to encode positions? 3 types.
    - absolute positional encoding
        $$PE = [sin(w_0 t), sin(w_1 t), sin(w_2 t), ..., sin(w_n t)]$$
        $$w_i = \frac{1}{10000^{d_{model}-1}}$$
        - WordEmbedding + PositionalEmbedding. use ADD not CONCAT.
        - when u visualize it
        [visual](1.5.abs_positional_embedding.png)
        - the longer the seq, values r less affected. not good for long seq.
        - attention requires pairwise dot-product. sin/cos cant func due to matrix multiplication.
        - how to solve?
    - BERT u learn a MLP to encode positional embedding: u init a [512, 768] matrix. concat w word embedding. then u learn the [512, 768]matrix. downside. u cant process longer than 512 tokens.
    - RNN positional encoding: add RNN to word embeeding. RNN learns the positions. but u cant parallel using RNN.
    - relative positional encoding. 
    - intra distances between tokens.
        - you construct a relative distance measure between token i and j. $$f|i-j|$$
        - variatons:
            - RoPe: if the encoding is embedded into Attention.
            - AliBi: if the encoding is using soft mask attention. u r penalizing the weights of tokens j farther away from token i.
            - 


    | Method                               | Formula                                                                                                                                           | What It Encodes                                   | Intuition                                                                                                                                                  |
    | ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | **Sinusoidal (Absolute)**            | $$PE(t)_i = \sin(t \cdot \omega_i),\quad \omega_i = 10000^{-i/d}$$                                                                                | Absolute position index                           | Each position gets a unique “wave signature.” Different frequencies let the model infer order (like reading a timestamp).                                  |
    | **Learned Absolute**                 | $$PE(t) = E_t$$                                                                                                                                   | Absolute position index                           | The model learns a table of positional vectors, like a “positional vocabulary.”                                                                            |
    | **Relative (Shaw et al.)**           | $$\text{score}*{ij} = Q_i K_j^\top + a*{i-j}$$                                                                                                    | Relative distance (i-j)                           | Instead of “where are they,” it learns “how far apart are they.” This matches how language works (“this word modifies the next one”).                      |
    | **Intra-token Distance (General)**   | $$PE(i,j) = f(\lvert i-j \rvert)$$                                                                                                                | Arbitrary distance functions                      | Can define *any* mapping from distance → bias. ALiBi and T5 are specific cases of this general framework.                                                  |
    | **ALiBi (Attention Linear Bias)**    | $$\text{score}_{ij} += w_h \cdot (i - j)$$                                                                                                        | Linear distance penalty                           | Far-away tokens get a *soft negative penalty* → similar to a **soft mask** that encourages attending to recent tokens. Generalizes to very long sequences. |
    | **RoPE (Rotary Position Embedding)** | $$Q_t^{rot}=R_tQ_t,\quad K_t^{rot}=R_tK_t$$ <br> $$R_t=\begin{bmatrix}\cos(\theta t)&-\sin(\theta t)\\sin(\theta t)&\cos(\theta t)\end{bmatrix}$$ | Encodes relative shift through geometric rotation | Positions rotate Q/K vectors in a circular space. Relative distances become differences in rotation. Elegant, smooth, natural for attention.               |
    | **T5 / DeBERTa (Relative Bias)**     | $$\text{score}*{ij} = Q_i K_j^\top + b*{\lvert i-j \rvert}$$                                                                                      | Bucketed distances                                | Distances are grouped into buckets. “Far but not too far” gets the same bucket. Stable and easy for NLP tasks.                                             |

### My Intuition
```
Sinusoidal / sin wave → every position = unique wave pattern  
Learned → positions learned like embeddings  
Shaw Relative → model learns "how far apart"  
ALiBi → soft distance mask (recent > far)  
RoPE → rotate queries/keys; relative emerges naturally  
T5/DeBERTa → distance buckets (coarse relative)  
Intra-token → any distance function you want  
```

- Length Extrapolation
    - def: when input seq too long. trainign seq 20k. inference seq 128k. model will collapse, bc it never sees a token at 3k position.
    - how to encode positiona info to the model so it can extrapolate to 1mm length?

        | Method                                     | Math (Core Formula)                                                                                                                        | Intuition (Why it works)                                                                    | Pros                                                                | Cons                                                                            |                               |                                      |
        | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ----------------------------- | ------------------------------------ |
        | **RPE (Relative Position Embedding)**      | $$\text{score}*{ij} = Q_i K_j^\top + a(i-j)$$                                                                                            | Encode **relative distance** rather than absolute position → natural for long sequences     | Good extrapolation; matches linguistic structure; stable            | Needs learned embeddings; bucketization often required                          |                               |                                      |
        | **RoPE (Rotary Positional Encoding)**      | $$Q_t^{rot}=R_tQ_t,; K_t^{rot}=R_tK_t$$ $$R_t=\begin{bmatrix}\cos(\theta t)&-\sin(\theta t) \ \sin(\theta t)&\cos(\theta t)\end{bmatrix}$$ | Encode positions as **rotations**; relative position emerges from angle differences         | Smooth, elegant; widely used (LLaMA/Qwen); supports relative shifts | High-frequency rotation grows too fast → breaks at very long context            |                               |                                      |
        | **ALiBi (Attention Linear Bias)**          | $$\text{score}_{ij} += w_h (i-j)$$                                                                                                         | Apply **linear distance penalty**, like a soft mask → model naturally prefers recent tokens | Perfect long-context extrapolation; no computation overhead         | No rotational/periodic structure; may weaken modeling of long-distance patterns |                               |                                      |
        | **Position Insertion (PI)**                | $$t' = \frac{L}{L_{\text{new}}}\cdot t$$                                                                                                   | Compress long sequence positions back into training length → model sees familiar ranges     | Extremely simple; works decently; improved by quick fine-tuning     | Pure interpolation → may distort long-range structure                           |                               |                                      |
        | **Base-N Positional Encoding**             | $$t = \sum_k d_k B^k$$ $$PE_k(t) = d_k$$                                                                                                    | Treat position as **digits** in multiple bases; digit count grows naturally                 | Infinite extrapolation; multi-scale; simple math                    | Discontinuous jumps; may need smoothing/finetuning                               |                               |                                      |
        | **NTK-aware RoPE Scaling**                 | $$\theta' = \theta / s$$                                                                                                                   | Slow down all RoPE frequencies → prevent angle explosion                                    | Easy, effective; improves long-context stability                    | Uniform scaling may under/over-correct different frequencies                    |                               |                                      |
        | **NTK by Parts (Frequency-segmented NTK)** | $$\theta'_i = \theta_i / s\cdot{\text{part}(i)}$$                                                                                              | Low freq → keep; mid freq → mild scale; high freq → strong scale                            | Better balance than global NTK; reduces distortion                  | More parameters; requires careful design                                        |                               |                                      |
        | **Dynamic NTK**                            | $$\theta'_i = \theta_i \cdot \frac{m}{\beta^{d/2-1}}$$ m is                                                                                     | Per-dimension adaptive scaling; distributes “frequency stress” smarter                      | Most adaptive NTK variant; very stable long-range                   | Harder to derive; more implementation steps                                     |                               |                                      |
        | **YaRN (Yet Another RoPE Extension)**      | $$softmax(\frac{q_m^T k_n}{t \sqrt{d_k}})$$                        so u scale query and key by $$\sqrt{1/t} $$ and $$\sqrt{1/t}=0.1 ln(s)+1$$                                                               | Combine **frequency scaling + controlled growth** → avoids over-shrink & under-rotation     | SOTA long-context RoPE scaling; robust and smooth                   | Implementation slightly more complex                                            |                               |                                      |


## intuitively
| Method      | Key Idea                                |
| ----------- | --------------------------------------- |
| RPE         | Learn distances                         |
| RoPE        | Encode angle rotation                   |
| NTK         | Slow down rotation globally             |
| NTK parts   | Slow down rotation by frequency segment |
| Dynamic NTK | Smart per-dimension scaling             |
| YaRN        | Best RoPE scaling (balanced + stable)   |
| ALiBi       | Linear recency bias = soft mask         |
| PI          | Compress indices back to training max   |
| Base-N      | Interpret positions as digits           |
| Intra-token | Arbitrary distance mapping              |


# Structure and Decoding
- llm architecture
    1. encoder only
        - pretraining. more for feature extraction
        - bi-directional. attention
        - BERT
    2. decoder only.
        - auto-regressive.
        - same as down stream. 
        - zero shot capability.
        - efficient training.
        - GPT, LLAMA, BLOOM, OPT. 
    3. encoder-decoder
        - encoder is bi-directional. decoder is auto-regressive.
        - T5, FlanT5, BART
    4. predix LM. special type of encoder-decoder model.
        - GLM, U-PaLM
    5. MoE: Mixture of Experts
        - N expert llm + G gate + add and normalize layer
        - Gates can be:
            - gate GLU
            - linear layer
            - domain mapping
            - expert-choice gating
            - attention router
        - softMoE:  N expert llm + G gate + merged FNN + add and normalize layer
        - e.g. deepseek v2/v3, mistral
- decoding strategies
    - greedy search [code](). always highest freq. too monotonic.
    - beam search [code]() might be too conservative.
    - top-k sampling
    - top-p sampling
    - temperature sampling
    - KPT
    - Best of N
    - Majority vote
    - self-consistency