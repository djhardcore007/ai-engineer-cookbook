# Tokenization

- why?
- 3 types of tokens: char, word, subwords
    - word: 1) OOV 2) Long tail effect 3) wont learn the similar meanings behind swim and swimming.
    - char: sequence length too long. expensive compute
    - subwords units.
- Subwords Methodologies
    -  1) first, u get 26 chars and punc, training corpus, and a required vocab size V. 2) split all tokens. e.g. cat [c_, _a_, _t, ca_, _at, cat].
    3) the question is: how to merge permutation units to |V| important units s.t. u have a best representation of the corpus? How to measure which subwords are the most important? You need to construct an importance score.
    - 3 methods
        - [BPE](https://arxiv.org/pdf/1508.07909) / Byte-level BPE.
            merge on frequency. count all freq of top units. merge all subwords that can reduce freq of each subwords and update vocab. start from top freq units (usually start from chars.)
            [code](1.1.bpe.py)
        - BBPE: Byte-level BPE.
            - intead of using string. use bytes.
            - pros:
                - BPE enables multi-lingual learnings.
                - compression
                - not just for text encoding and tokenization. can also work on images etc.
        - Wordpiece: find |V| subwords that gives the best log likelihood of the training corpus S.
            ```
            token x, y --> token z
            $$logP(z) - (logP(x)+logP(y)) = log\frac{P(x)}{P(x)P(y)}$$
            ```
        - Unigram Language Model
            - How to score if a token is important? A token is important if deleting the sub-units would cause more loss to the unigram model. This is still getting the top freq unigram model from a text corpus. 
            - keep single char. to avoid OOV.
            ```
            p(x) = \prod p(x_i). i = len(get_unigram(corpus)) 
            x_hat = arg max p(x)
            ```

- Comparisons
    - bpe and wordpiece: how to merge? from small vocab to big.
    - uigram lm: how to use lm to shrink vocab from big to small?

- tokenization is the roof of all evil, bc its next token prediction, u cant use llm for arithmetic computing.

# Embedding

- Why? word embedding. dense representation can get semantic mearnings between words. It's basically training MLP to get one-hot encoding vectors into dense vectors to represent word meanings. 
- lookup table `nn.Embedding(vocab_size, embedding_dim)` turn |V| vector into 512/768 dim.
- everything is a mapping. u can map a word into vector. u can map sentence into vector. u can also map tokens into high dim embedding space. it's more like how can u train that model?
- embedding 单射且同构
- Methods
    - one-hot
        - problems: dim explosion. one-hot encoding. every token is orthogonal. u cant learn the semantic meanings between words.
        - in some use cases, u can do one-hot encoding + PCA.
        - why we need to use one-hot? we turned discrete features into continuous. each feature is orthogonal
    - distributed representation: But u need to turn one-hot encoding features into Euclidean space so u can measure LP distance. dense representation is 1) easy to measur
    
    - word2vec: predict target word given context words.
        - cbow (contiuous bag of words)
            - Given the words around the blank, what word should be in the blank?
            - avg over context words' embeddings
            - ignores order.

            1. Context Words
                
                Context words: w_1, w_2, ..., w_c
                
                Target word: w_t  
            2. One-Hot Encoding

                Each word is a one-hot vector of size V (vocabulary size):
                $$
                            [
                            x_i \in \mathbb{R}^V
                            ]
                $$            
            3. Embedding Lookup

                Embedding matrix:
                $$
                [
                W \in \mathbb{R}^{V \times D}
                ]
                $$
                Word embedding for context word (w_i):
                $$
                [
                v_i = W^T x_i
                ]
                $$
                (Just selects the word’s embedding row.)
            
            4. Hidden layer: Average the Context Vectors
            $$

            [
            h = \frac{1}{C} \sum_{i=1}^{C} v_i
            ]
            $$
            5. Output Layer
            
                Second matrix:
                $$
                [
                W' \in \mathbb{R}^{D \times V}
                ]
                $$
                Scores for all words:
                $$
                [
                u = {W'}^T h
                ]
                $$
                
            6. Softmax Prediction
            $$
            [
            p(w_t \mid context)=
            \frac{\exp(u_t)}{\sum_{j=1}^{V} \exp(u_j)}
            ]
            $$
            Maximize the probability of the target word:
            $$
            [
            \max \log p(w_t \mid context)
            ]
            $$
            (Usually approximated with [negative sampling](https://en.wikipedia.org/wiki/Word2vec))
    - skip-gram: Predict context words FROM the target word.
    - fast-text: FastText still uses CBOW or Skip-Gram, but replaces the word embedding with the sum of its character n-gram vectors.
        
        Given word `playing` nrgams `n = 3`
        `<pl, pla, lay, ayi, yin, ing, ng>` + `<playing>`
        embedding of the word playing is now
        $$
        [
        v_{\text{playing}} = \sum_{g \in G} z_g
        ]
        $$
        Where:
        - (G) = set of character n-grams of the word
        - (z_g) = embedding vector of each n-gram

        Then apply cbow or skip gram on top.
        $$
        [
        \max \sum_{c \in context} \log p\left(c \mid \text{ngrams}(w_t)\right)
        ]
        $$

- word2vec how to accelerate?
    - hierarchical softmax
        - binary tree. complexity: |V| -> logV. 
        - higher freq words are located closer to the roots.
        - "greedy optimization"
        - cons: if the word is rare. u have to go deep into the branches.
    - negative sampling
        - def: a training trick used in word2vec (CBOW & Skip-Gram) to avoid computing a slow full softmax over the entire vocabulary.
        - why? faster and more efficient.
        - how?
            - The full softmax requires:
                $$
                [
                \text{sum over all } V \text{ words}
                ]
                $$
                If vocabulary size is big, this is extremely slow.
            - Negative sampling replaces this expensive softmax with a binary classification task. how to construct?
                - 1 positive example  (the true target/context word)
                    ```
                    (target = "cat", context = "cute")
                    ```
                - K negative examples (random wrong words)
                    ```
                    ("cat", "banana")
                    ("cat", "engine")
                    ("cat", "chair")
                    ```
            - maths
            
                Positive example:
                $$
                [
                \log \sigma (v_{context}^\top v_{target})
                ]
                $$
                Negative examples:
                $$
                [
                \sum_{i=1}^{K} \log \sigma (-v_{neg_i}^\top v_{target})
                ]
                $$
                Total loss:
                $$
                [
                L = \log \sigma (v_{+}^\top v_{t})

                * \sum_{i=1}^{K} \log \sigma (-v_{neg_i}^\top v_{t})
                ]
                $$
                Where:
                - (\sigma) = sigmoid
                - (v_+) = embedding of the real context word
                - (v_{neg_i}) = embeddings of negative words


# Attention

- Why?
- self-attention
- multi-head self-attention
- sparse attention
- linear attention
- KV cache
- multi-head attention
- multi-query attention
- grouped-query attention
- multihead latent attention

# FFN Add LN

- why FFN?
- why add residual?
- why layer norm?
- how to do layer norm correctly?
- why batch norm?
- what types of activations
    - sigmoid
    - tanh
    - relu
    - leaky relu
    - elu
    - swish
    - swiglu
    - softmax

# Postional Encoding
- why?
- absolute positional encoding
- relative positional encoding
- RoPE
- ALiBi
- 长度外推优化

# Structure and Decoding
- llm architecture
- decoding strategies
    - greedy search
    - beam search
    - top-k sampling
    - top-p sampling
    - temperature sampling
    - KPT
    - Best of N
    - Majority vote
    - self-consistency